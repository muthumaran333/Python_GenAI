{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73d7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 Embeddings\n",
      "02 Embeddings\n",
      "What are Embeddings?\n",
      "Embeddings are dense vector representations of data in a continuous vector space. They capture\n",
      "semantic meaning and contextual relationships by mapping discrete entities (like words, sentences,\n",
      "images, or user behaviors) to points in a multi-dimensional space where similar items are positioned\n",
      "closer together.\n",
      "Unlike traditional sparse representations (like one-hot encoding), embeddings are:\n",
      "Dense: They use a fixed number of dimensions to represent information efficiently\n",
      "Learned: They're derived from data rather than manually engineered\n",
      "Continuous: They exist in a smooth vector space allowing for mathematical operations\n",
      "Semantic: They capture meaningful relationships between entities\n",
      "Real-World Example\n",
      "Consider how we might represent colors. A traditional representation might use discrete RGB values\n",
      "(255, 0, 0) for red. An embedding approach would learn that \"scarlet\" and \"crimson\" should be\n",
      "positioned close to \"red\" in vector space, while \"navy\" and \"azure\" would cluster near \"blue.\" This\n",
      "proximity in the embedding space reflects their semantic similarity.\n",
      "How Embeddings Work\n",
      "Embeddings transform discrete data into continuous vector representations through a process of\n",
      "learning from context and relationships within data.\n",
      "The Training Process\n",
      "1. Initialize: Start with random vectors for each entity (word, image, etc.)\n",
      "2. Define an objective: Create a task where predicting context is required\n",
      "For words: Predict surrounding words (Word2Vec)\n",
      "For sentences: Predict whether one sentence follows another (Sentence-BERT)\n",
      "For images: Predict whether images are similar (CLIP)\n",
      "3. Train: Update embeddings through gradient descent to minimize prediction error\n",
      "4. Extract: The resulting vectors become your embeddings\n",
      "Vector Operations\n",
      "Once trained, embeddings enable powerful vector operations:\n",
      "Similarity: Calculate cosine similarity between vectors to find related items\n",
      "Analogy: Famous example: \"king\" - \"man\" + \"woman\" ≈ \"queen\"\n",
      "Clustering: Group similar entities based on embedding proximity\n",
      "Dimensionality\n",
      "Most embedding models use vectors with dimensions ranging from 100-1024. Higher dimensions can\n",
      "capture more nuanced relationships but require more data to train effectively and more computational\n",
      "resources to use.\n",
      "Real-World Example\n",
      "Nikhil Sharma Page 102 Embeddings\n",
      "Consider a product recommendation system. When a user views a red cotton t-shirt, the system doesn't\n",
      "just recommend identical items. Instead, it:\n",
      "1. Converts the product into its embedding vector\n",
      "2. Finds other products with similar embeddings\n",
      "3. Recommends products that may differ in color or exact style but share underlying characteristics\n",
      "the user might appreciate\n",
      "The system learns these relationships from user behavior patterns rather than explicit rules.\n",
      "Use Cases of Embeddings\n",
      "Natural Language Processing\n",
      "Semantic search: Find documents based on meaning rather than exact keywords\n",
      "Text classification: Categorize documents into topics or sentiment\n",
      "Machine translation: Convert words/sentences between languages\n",
      "Question answering: Understand the intent behind questions\n",
      "Computer Vision\n",
      "Image retrieval: Find visually similar images\n",
      "Face recognition: Match faces across different images\n",
      "Video understanding: Classify actions or scenes\n",
      "Recommender Systems\n",
      "Content-based filtering: Recommend items similar to what users have liked\n",
      "Collaborative filtering: Find users with similar preferences\n",
      "Hybrid approaches: Combine multiple embedding types\n",
      "Graph Analysis\n",
      "Node embeddings: Represent entities in a network\n",
      "Link prediction: Identify potential new connections\n",
      "Community detection: Find clusters of related entities\n",
      "Multimodal Applications\n",
      "Text-to-image generation: Transform text descriptions into visual elements\n",
      "Cross-modal retrieval: Find images matching text queries or vice-versa\n",
      "Real-World Example\n",
      "A healthcare organization might use embeddings to:\n",
      "1. Convert patient medical records to embeddings\n",
      "2. Identify clusters of patients with similar conditions\n",
      "3. For a new patient, quickly find similar historical cases\n",
      "4. Use those similar cases to recommend treatment plans based on what worked previously\n",
      "This approach can find subtle patterns that rule-based systems might miss.\n",
      "Implementing Embeddings with Hugging Face (Free\n",
      "Models)\n",
      "Nikhil Sharma Page 202 Embeddings\n",
      "Text Embeddings\n",
      "Sentence Transformers\n",
      "from sentence_transformers import SentenceTransformer\n",
      "# Load a pre-trained model\n",
      "model = SentenceTransformer('all-MiniLM-L6-v2') # Small, efficient model (384\n",
      "dimensions)\n",
      "# Create embeddings for some text\n",
      "sentences = [\n",
      "\"This patient shows signs of diabetes\",\n",
      "\"The patient exhibits symptoms consistent with diabetes mellitus\",\n",
      "\"This user frequently purchases outdoor equipment\"\n",
      "]\n",
      "embeddings = model.encode(sentences)\n",
      "print(f\"Shape of embeddings: {embeddings.shape}\") # [3, 384]\n",
      "# Calculate similarity between first two sentences (medical) vs third (retail)\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "similarity_matrix = cosine_similarity(embeddings)\n",
      "print(f\"Similarity between sentences 1 and 2: {similarity_matrix[0][1]:.4f}\") #\n",
      "High similarity\n",
      "print(f\"Similarity between sentences 1 and 3: {similarity_matrix[0][2]:.4f}\") #\n",
      "Low similarity\n",
      "Using Transformers Directly\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "import torch\n",
      "# Load BERT model and tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = AutoModel.from_pretrained('bert-base-uncased')\n",
      "# Prepare text\n",
      "text = \"Machine learning is transforming healthcare\"\n",
      "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
      "# Generate embeddings\n",
      "with torch.no_grad():\n",
      "outputs = model(\u0000\u0000inputs)\n",
      "# Use CLS token as sentence embedding\n",
      "embeddings = outputs.last_hidden_state[:, 0, :]\n",
      "print(f\"Embedding shape: {embeddings.shape}\") # [1, 768]\n",
      "Image Embeddings\n",
      "Nikhil Sharma Page 302 Embeddings\n",
      "from transformers import CLIPProcessor, CLIPModel\n",
      "import requests\n",
      "from PIL import Image\n",
      "# Load model and processor\n",
      "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "# Load and process an image\n",
      "url = \"http\u0000\u0000\u0000images.cocodataset.org/val2017/000000039769.jpg\"\n",
      "image = Image.open(requests.get(url, stream=True).raw)\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# Generate image embeddings\n",
      "with torch.no_grad():\n",
      "outputs = model.get_image_features(\u0000\u0000inputs)\n",
      "image_embedding = outputs\n",
      "print(f\"Image embedding shape: {image_embedding.shape}\") # [1, 512]\n",
      "Multimodal Embeddings (Text and Image)\n",
      "from transformers import CLIPProcessor, CLIPModel\n",
      "# Load CLIP model and processor\n",
      "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "# Prepare image and text\n",
      "url = \"http\u0000\u0000\u0000images.cocodataset.org/val2017/000000039769.jpg\"\n",
      "image = Image.open(requests.get(url, stream=True).raw)\n",
      "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
      "# Process inputs\n",
      "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
      "# Get similarity scores\n",
      "with torch.no_grad():\n",
      "outputs = model(\u0000\u0000inputs)\n",
      "logits_per_text = outputs.logits_per_text # Text-image similarity\n",
      "print(f\"Similarity scores: {logits_per_text}\") # Higher score for correct match\n",
      "Building a Semantic Search System\n",
      "import numpy as np\n",
      "from sentence_transformers import SentenceTransformer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "# Sample database of documents\n",
      "Nikhil Sharma Page 402 Embeddings\n",
      "documents = [\n",
      "\"Artificial intelligence is revolutionizing healthcare\",\n",
      "\"Machine learning models require significant data for training\",\n",
      "\"Neural networks have multiple layers of computational nodes\",\n",
      "\"Healthcare costs continue to rise in many countries\",\n",
      "\"Data privacy is a major concern in medical applications\"\n",
      "]\n",
      "# Create model and embeddings\n",
      "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "document_embeddings = model.encode(documents)\n",
      "# Function to search documents\n",
      "def semantic_search(query, top_k=2):\n",
      "# Create embedding for the query\n",
      "query_embedding = model.encode([query])\n",
      "# Calculate similarity scores\n",
      "similarity_scores = cosine_similarity(query_embedding, document_embeddings)[0]\n",
      "# Get top results\n",
      "top_indices = np.argsort(similarity_scores)[\u0000 -1][:top_k]\n",
      "# Return results\n",
      "results = []\n",
      "for idx in top_indices:\n",
      "results.append({\n",
      "\"document\": documents[idx],\n",
      "\"score\": similarity_scores[idx]\n",
      "})\n",
      "return results\n",
      "# Example search\n",
      "results = semantic_search(\"AI in medicine\", top_k=2)\n",
      "for i, result in enumerate(results):\n",
      "print(f\"Result {i+1}: {result['document']} (Score: {result['score']:.4f})\")\n",
      "Advanced Concepts in Embeddings\n",
      "Embedding Spaces and Transfer Learning\n",
      "Embeddings trained on one task often transfer well to related tasks. This is because the learned vector\n",
      "space captures fundamental patterns that generalize across domains. A model trained on general\n",
      "English text can often produce useful embeddings for specialized domains like legal or medical text with\n",
      "minimal fine-tuning.\n",
      "Contextual vs. Static Embeddings\n",
      "Early embedding models like Word2Vec produced static embeddings where each word has a single\n",
      "vector regardless of context. Modern transformer-based models generate contextual embeddings where\n",
      "the same word receives different vectors based on surrounding context:\n",
      "Nikhil Sharma Page 502 Embeddings\n",
      "\"The bank denied my loan application.\" → financial meaning\n",
      "\"I sat by the bank of the river.\" → geographical meaning\n",
      "Embedding Visualization\n",
      "Tools like TensorBoard or UMAP allow visualization of high-dimensional embeddings in 2D or 3D space,\n",
      "revealing clusters and relationships:\n",
      "from sklearn.manifold import TSNE\n",
      "import matplotlib.pyplot as plt\n",
      "# Create t-SNE model\n",
      "tsne = TSNE(n_components=2, random_state=42)\n",
      "# Fit and transform embeddings to 2D\n",
      "embeddings_2d = tsne.fit_transform(document_embeddings)\n",
      "# Plot\n",
      "plt.figure(figsize=(10, 8))\n",
      "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
      "# Add labels\n",
      "for i, doc in enumerate(documents):\n",
      "plt.annotate(doc[:20] + \"\u0000\u0000\u0000\", (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
      "plt.title(\"t-SNE visualization of document embeddings\")\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "Fine-tuning Embeddings\n",
      "General-purpose embeddings can be fine-tuned for specific domains:\n",
      "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
      "from torch.utils.data import DataLoader\n",
      "# Create a training dataset of similar and dissimilar pairs\n",
      "train_examples = [\n",
      "InputExample(texts=['diabetes symptoms', 'signs of high blood sugar'],\n",
      "label=0.9),\n",
      "InputExample(texts=['heart disease', 'cardiac condition'], label=0.8),\n",
      "InputExample(texts=['diabetes symptoms', 'car maintenance'], label=0.1)\n",
      "]\n",
      "# Create dataloader\n",
      "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
      "# Load model\n",
      "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "# Train with cosine similarity loss\n",
      "Nikhil Sharma Page 602 Embeddings\n",
      "train_loss = losses.CosineSimilarityLoss(model)\n",
      "model.fit(\n",
      "train_objectives=[(train_dataloader, train_loss)],\n",
      "epochs=1,\n",
      "warmup_steps=100\n",
      ")\n",
      "Ethical Considerations\n",
      "Embeddings can inherit and amplify biases present in training data. For example:\n",
      "Word embeddings trained on news articles have shown gender biases (associating \"doctor\" with\n",
      "male terms and \"nurse\" with female terms)\n",
      "Face recognition embeddings have demonstrated racial biases in accuracy rates\n",
      "Mitigation strategies include:\n",
      "Careful dataset curation and balancing\n",
      "Debiasing techniques during or after training\n",
      "Regular auditing of embedding-based systems\n",
      "Transparency about limitations\n",
      "Future Directions\n",
      "Multimodal embeddings: Unified representations across text, images, audio, and video\n",
      "Sparse embeddings: Combining benefits of sparse and dense representations for better\n",
      "interpretability\n",
      "Hierarchical embeddings: Representing information at multiple levels of abstraction\n",
      "Domain-specific architectures: Models optimized for particular fields like chemistry or genomics\n",
      "Embedding compression: Creating smaller, more efficient embeddings for resource-constrained\n",
      "environments\n",
      "Foundation model embeddings: Leveraging increasingly powerful large language models to\n",
      "generate rich, context-aware embeddings\n",
      "Embeddings continue to evolve as a fundamental building block of modern machine learning systems,\n",
      "enabling machines to understand and process information in ways that more closely resemble human\n",
      "conceptual thinking.\n",
      "Nikhil Sharma Page 7\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_file = 'knowledge-base/02 Embeddings.pdf'\n",
    "\n",
    "with pdfplumber.open(pdf_file) as pdf:\n",
    "    extracted_text = ''\n",
    "    for page in pdf.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848199be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pdf_file = 'knowledge-base/02 Embeddings.pdf'\n",
    "pdf = pdfplumber.open(pdf_file)\n",
    "\n",
    "page = pdf.pages[0]\n",
    "\n",
    "table_data = page.extract_tables()\n",
    "\n",
    "pprint(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac0cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_table_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        pages = pdf.pages\n",
    "        data = []\n",
    "        for page in pages:\n",
    "            table = page.extract_table()\n",
    "            if table:\n",
    "                data.extend(table)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_excel(data, excel_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(excel_path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = 'knowledge-base/02 Embeddings.pdf'\n",
    "    excel_path = 'test.csv'\n",
    "\n",
    "    extracted_data = extract_table_from_pdf(pdf_path)\n",
    "    save_to_excel(extracted_data, excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99849e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level=1, title=02 Embeddings, page=1\n",
      "level=1, title=What are Embeddings?, page=1\n",
      "level=2, title=Real-World Example, page=1\n",
      "level=2, title=How Embeddings Work, page=1\n",
      "level=3, title=The Training Process, page=1\n",
      "level=3, title=Vector Operations, page=1\n",
      "level=3, title=Dimensionality, page=1\n",
      "level=3, title=Real-World Example, page=1\n",
      "level=2, title=Use Cases of Embeddings, page=2\n",
      "level=3, title=Natural Language Processing, page=2\n",
      "level=3, title=Computer Vision, page=2\n",
      "level=3, title=Recommender Systems, page=2\n",
      "level=3, title=Graph Analysis, page=2\n",
      "level=3, title=Multimodal Applications, page=2\n",
      "level=3, title=Real-World Example, page=2\n",
      "level=2, title=Implementing Embeddings with Hugging Face (Free Models), page=2\n",
      "level=3, title=Text Embeddings, page=3\n",
      "level=4, title=Sentence Transformers, page=3\n",
      "level=4, title=Using Transformers Directly, page=3\n",
      "level=3, title=Image Embeddings, page=3\n",
      "level=3, title=Multimodal Embeddings (Text and Image), page=4\n",
      "level=2, title=Building a Semantic Search System, page=4\n",
      "level=2, title=Advanced Concepts in Embeddings, page=5\n",
      "level=3, title=Embedding Spaces and Transfer Learning, page=5\n",
      "level=3, title=Contextual vs. Static Embeddings, page=5\n",
      "level=3, title=Embedding Visualization, page=6\n",
      "level=3, title=Fine-tuning Embeddings, page=6\n",
      "level=2, title=Ethical Considerations, page=7\n",
      "level=2, title=Future Directions, page=7\n",
      "level=1, title=[Bookmark] New Bookmark, page=1\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdftypes import PDFObjRef, resolve1\n",
    "from pdfminer.psparser import PSLiteral\n",
    "\n",
    "pdf_file = 'knowledge-base/02 Embeddings.pdf'\n",
    "\n",
    "def resolve_dest(dest, doc):\n",
    "    \"\"\"Resolve a PDF destination object into its underlying data.\"\"\"\n",
    "    if isinstance(dest, str):\n",
    "        dest = resolve1(doc.get_dest(dest))\n",
    "    elif isinstance(dest, PSLiteral):\n",
    "        dest = resolve1(doc.get_dest(dest.name))\n",
    "    if isinstance(dest, dict):\n",
    "        dest = dest.get('D')\n",
    "    if isinstance(dest, PDFObjRef):\n",
    "        dest = dest.resolve()\n",
    "    return dest\n",
    "\n",
    "with pdfplumber.open(pdf_file) as pdf:\n",
    "    pdf_doc = pdf.doc\n",
    "    outlines = pdf_doc.get_outlines()\n",
    "\n",
    "    # Map page IDs to page numbers\n",
    "    pages = {page.pageid: pageno for (pageno, page)\n",
    "             in enumerate(PDFPage.create_pages(pdf_doc), 1)}\n",
    "    \n",
    "    for (level, title, dest, a, se) in outlines:\n",
    "        pageno = None\n",
    "        \n",
    "        if dest:\n",
    "            dest = resolve_dest(dest, pdf_doc)\n",
    "            if isinstance(dest, list) and len(dest) > 0 and hasattr(dest[0], 'objid'):\n",
    "                pageno = pages.get(dest[0].objid)\n",
    "        \n",
    "        elif a:\n",
    "            if isinstance(a, dict):\n",
    "                subtype = a.get('S')\n",
    "                if subtype and repr(subtype) == \"/'GoTo'\" and a.get('D'):\n",
    "                    dest = resolve_dest(a['D'], pdf_doc)\n",
    "                    if isinstance(dest, list) and len(dest) > 0 and hasattr(dest[0], 'objid'):\n",
    "                        pageno = pages.get(dest[0].objid)\n",
    "\n",
    "        print(f\"level={level}, title={title}, page={pageno}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
